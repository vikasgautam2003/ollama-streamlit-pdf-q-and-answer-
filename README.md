# ğŸ“„ Ollama PDF Q & Ans

An AI-powered PDF Question-Answering and Summarization app built with **Streamlit** and **Ollama (TinyLlama)**.  
This project demonstrates how to integrate **local LLMs** with a modern web UI for document understanding.  

---

## ğŸš€ Features  
- ğŸ“‘ Upload any PDF document  
- ğŸ¤– Ask questions about the document content  
- ğŸ“ Generate AI-powered summaries  
- âš¡ Powered by [Ollama](https://ollama.ai/) running **TinyLlama** locally  
- ğŸŒ Web UI built with [Streamlit](https://streamlit.io/)  

---

## ğŸ› ï¸ Tech Stack  
- **LLM Backend** â†’ Ollama (TinyLlama)  
- **Frontend** â†’ Streamlit  
- **Containerization** â†’ Docker  
- **Language** â†’ Python 3.10+  

---

## ğŸ“‚ Project Structure  
â”œâ”€â”€ app.py / pdfSummary.py # Main Streamlit app
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ start.sh # Startup script (Ollama + Streamlit)
â”œâ”€â”€ Dockerfile # Container setup
â”œâ”€â”€ notes.txt # Development notes
â””â”€â”€ README.md # Project documentation




---

## âš™ï¸ Setup Instructions  

### ğŸ”¹ Option A: Run Locally  
1. Install [Ollama](https://ollama.ai/)  
2. Pull the TinyLlama model:  
   ```bash
   ollama pull tinyllama



Install Python dependencies:

pip install -r requirements.txt


Run the app:

streamlit run pdfSummary.py --server.port 8501 --server.address 0.0.0.0

ğŸ”¹ Option B: Run with Docker

Build the image:

docker build -t ollama_pdf_app .


Run the container:

docker run -p 8501:8501 ollama_pdf_app

ğŸ¯ Use Case

This project is ideal for:

Showcasing LLM integration skills

Building AI-powered portfolio projects

Demonstrating document question answering

ğŸ“¸ Demo (Screenshots / Video)

ğŸ‘‰ Add screenshots of your UI here
ğŸ‘‰ (Optional) Upload a short demo video / GIF

ğŸ’¡ Future Improvements

Support for larger models (Mistral, Llama 3)

Deploy lightweight version on free hosting (Render / Railway)

Hugging Face Space demo

ğŸ‘¨â€ğŸ’» Author

Vikas Gautam
